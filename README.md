# RL Trading Agent ğŸ¤–ğŸ“ˆ

A sophisticated Reinforcement Learning system for automated trading, implementing state-of-the-art algorithms for continuous action space trading decisions.

## ğŸ¯ Features

- **Multiple RL Algorithms**: PPO, SAC, and DQN implementations
- **LSTM/GRU Networks**: Time-series aware neural architectures
- **Real-time Trading**: Live simulation capabilities
- **Comprehensive Backtesting**: Historical performance evaluation
- **Advanced Preprocessing**: Technical indicators and feature engineering
- **Experiment Tracking**: Integration with Weights & Biases and TensorBoard
- **Modular Design**: Clean, extensible architecture

## ğŸ“‹ Requirements

- Python 3.10.11
- CUDA-capable GPU (optional but recommended)
- 16GB+ RAM recommended

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd RL_Project

# Install dependencies
pip install -r requirements.txt

# Install TA-Lib (platform specific)
# For Windows:
# Download from https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib
# pip install TA_Lib-0.4.24-cp310-cp310-win_amd64.whl

# For Linux/Mac:
# brew install ta-lib  # Mac
# sudo apt-get install ta-lib  # Ubuntu
```

### 2. Prepare Data

```bash
# Download and preprocess data
python main.py prepare --ticker AAPL --start 2023-01-01 --end 2024-01-01
```

### 3. Train Agent

```bash
# Train with default configuration
python main.py train --config configs/config.yaml

# Train with GPU
python main.py train --config configs/config.yaml --device cuda

# Resume training from checkpoint
python main.py train --resume models/checkpoints/model_50000.zip
```

### 4. Evaluate Performance

```bash
# Evaluate trained model
python main.py evaluate --model models/best/model.zip --episodes 10 --plot

# Run backtesting
python main.py backtest --model models/best/model.zip --start 2023-06-01 --end 2023-12-31
```

## ğŸ“ Project Structure

```
RL_Project/
â”‚
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ raw/               # Raw market data
â”‚   â””â”€â”€ processed/         # Preprocessed features
â”‚
â”œâ”€â”€ envs/                  # Trading environments
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trading_env.py     # Custom Gym environment
â”‚
â”œâ”€â”€ models/                # Model architectures and agents
â”‚   â”œâ”€â”€ agent.py          # RL agent implementation
â”‚   â””â”€â”€ network.py        # Neural network architectures
â”‚
â”œâ”€â”€ scripts/              # Training and evaluation scripts
â”‚   â”œâ”€â”€ train.py         # Main training loop
â”‚   â””â”€â”€ evaluate.py      # Evaluation and backtesting
â”‚
â”œâ”€â”€ utils/               # Utility functions
â”‚   â”œâ”€â”€ logger.py       # Logging utilities
â”‚   â”œâ”€â”€ replay_buffer.py # Experience replay
â”‚   â””â”€â”€ preprocessing.py # Data processing
â”‚
â”œâ”€â”€ configs/            # Configuration files
â”‚   â””â”€â”€ config.yaml    # Main configuration
â”‚
â”œâ”€â”€ results/           # Output directory
â”‚   â”œâ”€â”€ checkpoints/  # Model checkpoints
â”‚   â”œâ”€â”€ logs/        # Training logs
â”‚   â””â”€â”€ plots/       # Visualizations
â”‚
â”œâ”€â”€ main.py          # Main entry point
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md       # Documentation
```

## âš™ï¸ Configuration

Edit `configs/config.yaml` to customize:

- **Environment**: Trading window, transaction costs, position limits
- **Data**: Ticker symbols, date ranges, technical indicators
- **Agent**: Algorithm choice (PPO/SAC), network architecture
- **Training**: Learning rates, batch sizes, training duration